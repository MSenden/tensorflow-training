{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with TensorFlow's `Dataset` API (continuation)\n",
    "\n",
    "In this notebook we will learn how to divide the dataset over the ranks in distributed training.\n",
    "\n",
    "The following steps were done on one of the previous notebooks. If necessary they can be run again on a new cell.\n",
    "```bash\n",
    "wget https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_setosa.csv\n",
    "grep setosa iris.csv >> iris_setosa.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_versic.csv\n",
    "grep versicolor iris.csv >> iris_versic.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_virgin.csv\n",
    "grep virginica iris.csv >> iris_virgin.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipcmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPCluster is ready! (5 seconds)\n"
     ]
    }
   ],
   "source": [
    "%ipcluster start -n 2 --mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow.keras as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:2]: \u001b[0m'2.3.0'"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2020-08-12T12:55:12.230371",
      "data": {},
      "engine_id": 0,
      "engine_uuid": "2ae3f4a5-ad7865868e423780c5aff437",
      "error": null,
      "execute_input": "tf.version.VERSION\n",
      "execute_result": {
       "data": {
        "text/plain": "'2.3.0'"
       },
       "execution_count": 2,
       "metadata": {}
      },
      "follow": [],
      "msg_id": "81bd2886-6cc40231a11b07526aea52f9_3",
      "outputs": [],
      "received": "2020-08-12T12:55:12.232983",
      "started": "2020-08-12T12:55:12.223034",
      "status": "ok",
      "stderr": "",
      "stdout": "",
      "submitted": "2020-08-12T12:55:12.220898"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:2]: \u001b[0m'2.3.0'"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2020-08-12T12:55:12.229884",
      "data": {},
      "engine_id": 1,
      "engine_uuid": "4aca940e-f793cc3f14c1690df137ca6b",
      "error": null,
      "execute_input": "tf.version.VERSION\n",
      "execute_result": {
       "data": {
        "text/plain": "'2.3.0'"
       },
       "execution_count": 2,
       "metadata": {}
      },
      "follow": [],
      "msg_id": "81bd2886-6cc40231a11b07526aea52f9_4",
      "outputs": [],
      "received": "2020-08-12T12:55:12.231976",
      "started": "2020-08-12T12:55:12.223108",
      "status": "ok",
      "stderr": "",
      "stdout": "",
      "submitted": "2020-08-12T12:55:12.221044"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def parse_columns(*row, classes):\n",
    "    \"\"\"Convert the string classes to one-hot encoded:\n",
    "    setosa     -> [1, 0, 0]\n",
    "    virginica  -> [0, 1, 0]\n",
    "    versicolor -> [0, 0, 1]\n",
    "    \"\"\"\n",
    "    features = tf.convert_to_tensor(row[:4])\n",
    "    label_int = tf.where(tf.equal(classes, row[4]))\n",
    "    label = tf.one_hot(label_int, 3)\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def get_csv_dataset(filename):\n",
    "    return tf.data.experimental.CsvDataset(filename, header=True,\n",
    "                                           record_defaults=[tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.string])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Shards <a id='using_shards'></a>\n",
    "\n",
    "\n",
    "Let's consider a distributed training with two ranks to see what happens with the data on each worker. In distributed training one can use [`tf.data.Dataset.shard`]( https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard) to divide the dataset over the ranks, otherwise the same data might be sent to each of the workers.\n",
    "\n",
    "Let's consider:\n",
    " * `tf.data.Dataset.list_files` with `shuffle=True`.\n",
    " * `tf.data.Dataset.list_files` with `shuffle=False`.\n",
    " * Shard before interleaving.\n",
    " * Shard after interleaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "hvd.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:9]: \u001b[0m(2, 0)"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2020-08-12T12:56:47.068969",
      "data": {},
      "engine_id": 0,
      "engine_uuid": "2ae3f4a5-ad7865868e423780c5aff437",
      "error": null,
      "execute_input": "hvd.size(), hvd.rank()\n",
      "execute_result": {
       "data": {
        "text/plain": "(2, 0)"
       },
       "execution_count": 9,
       "metadata": {}
      },
      "follow": [],
      "msg_id": "81bd2886-6cc40231a11b07526aea52f9_19",
      "outputs": [],
      "received": "2020-08-12T12:56:47.071237",
      "started": "2020-08-12T12:56:47.066617",
      "status": "ok",
      "stderr": "",
      "stdout": "",
      "submitted": "2020-08-12T12:56:47.064429"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:9]: \u001b[0m(2, 1)"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2020-08-12T12:56:47.069385",
      "data": {},
      "engine_id": 1,
      "engine_uuid": "4aca940e-f793cc3f14c1690df137ca6b",
      "error": null,
      "execute_input": "hvd.size(), hvd.rank()\n",
      "execute_result": {
       "data": {
        "text/plain": "(2, 1)"
       },
       "execution_count": 9,
       "metadata": {}
      },
      "follow": [],
      "msg_id": "81bd2886-6cc40231a11b07526aea52f9_20",
      "outputs": [],
      "received": "2020-08-12T12:56:47.072626",
      "started": "2020-08-12T12:56:47.066857",
      "status": "ok",
      "stderr": "",
      "stdout": "",
      "submitted": "2020-08-12T12:56:47.064577"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "hvd.size(), hvd.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "x: [5.1 3.5 1.4 0.2]    y: [[[1. 0. 0.]]]\n",
      "x: [4.9 3.  1.4 0.2]    y: [[[1. 0. 0.]]]\n",
      "x: [4.7 3.2 1.3 0.2]    y: [[[1. 0. 0.]]]\n",
      "x: [4.6 3.1 1.5 0.2]    y: [[[1. 0. 0.]]]\n",
      "x: [5.  3.6 1.4 0.2]    y: [[[1. 0. 0.]]]\n",
      "[stdout:1] \n",
      "x: [7.  3.2 4.7 1.4]    y: [[[0. 0. 1.]]]\n",
      "x: [6.4 3.2 4.5 1.5]    y: [[[0. 0. 1.]]]\n",
      "x: [6.9 3.1 4.9 1.5]    y: [[[0. 0. 1.]]]\n",
      "x: [5.5 2.3 4.  1.3]    y: [[[0. 0. 1.]]]\n",
      "x: [6.5 2.8 4.6 1.5]    y: [[[0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "dataset = tf.data.Dataset.list_files(['iris_setosa.csv',\n",
    "                                      'iris_versic.csv'],\n",
    "                                      shuffle=False)  # `shuffle=False` to ensure that for both ranks the files are in the same order.\n",
    "dataset = dataset.interleave(get_csv_dataset,\n",
    "                             cycle_length=2,\n",
    "                             block_length=1,\n",
    "                             num_parallel_calls=1)\n",
    "dataset = dataset.shard(hvd.size(), hvd.rank())\n",
    "dataset = dataset.map(lambda *row: parse_columns(*row, classes=['setosa', 'virginica', 'versicolor']))\n",
    "\n",
    "for x, y in dataset.take(5):\n",
    "    print(f'features: {x}    label: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-multigpu",
   "language": "python",
   "name": "tf-multigpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
