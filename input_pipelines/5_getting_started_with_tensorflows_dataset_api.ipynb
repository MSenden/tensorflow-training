{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with TensorFlow's `Dataset` API (continuation)\n",
    "\n",
    "In this notebook we continue on how to divide the dataset over the ranks in distributed training. We will combine sharding and interleaving.\n",
    "\n",
    "The following steps were done on one of the previous notebooks. If necessary they can be run again on a new cell.\n",
    "```bash\n",
    "wget https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_setosa.csv\n",
    "grep setosa iris.csv >> iris_setosa.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_versic.csv\n",
    "grep versicolor iris.csv >> iris_versic.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_virgin.csv\n",
    "grep virginica iris.csv >> iris_virgin.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipcmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPCluster is ready! (6 seconds)\n"
     ]
    }
   ],
   "source": [
    "%ipcluster start -n 2 --mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow.keras as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:3]: \u001b[0m'2.3.0'"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2020-08-26T21:11:37.519199",
      "data": {},
      "engine_id": 1,
      "engine_uuid": "a00a079c-e44a373b7eb530adb0ab9060",
      "error": null,
      "execute_input": "tf.version.VERSION\n",
      "execute_result": {
       "data": {
        "text/plain": "'2.3.0'"
       },
       "execution_count": 3,
       "metadata": {}
      },
      "follow": [],
      "msg_id": "c3e015e7-742f870be687028229e97c1d_5",
      "outputs": [],
      "received": "2020-08-26T21:11:37.520996",
      "started": "2020-08-26T21:11:37.517138",
      "status": "ok",
      "stderr": "",
      "stdout": "",
      "submitted": "2020-08-26T21:11:37.515397"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --target 1\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def parse_columns(*row, classes):\n",
    "    \"\"\"Convert the string classes to one-hot encoded:\n",
    "    setosa     -> [1, 0, 0]\n",
    "    virginica  -> [0, 1, 0]\n",
    "    versicolor -> [0, 0, 1]\n",
    "    \"\"\"\n",
    "    features = tf.stack(row[:4])\n",
    "    label_int = tf.where(tf.equal(classes, row[4]))\n",
    "    label = tf.one_hot(label_int, 3)\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def get_csv_dataset(filename):\n",
    "    return tf.data.experimental.CsvDataset(filename, header=True,\n",
    "                                           record_defaults=[tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "hvd.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "features: [5.1 3.5 1.4 0.2]    label: [[[1. 0. 0.]]]\n",
      "features: [4.9 3.  1.4 0.2]    label: [[[1. 0. 0.]]]\n",
      "features: [4.7 3.2 1.3 0.2]    label: [[[1. 0. 0.]]]\n",
      "features: [4.6 3.1 1.5 0.2]    label: [[[1. 0. 0.]]]\n",
      "features: [5.  3.6 1.4 0.2]    label: [[[1. 0. 0.]]]\n",
      "[stdout:1] \n",
      "features: [7.  3.2 4.7 1.4]    label: [[[0. 0. 1.]]]\n",
      "features: [6.4 3.2 4.5 1.5]    label: [[[0. 0. 1.]]]\n",
      "features: [6.9 3.1 4.9 1.5]    label: [[[0. 0. 1.]]]\n",
      "features: [5.5 2.3 4.  1.3]    label: [[[0. 0. 1.]]]\n",
      "features: [6.5 2.8 4.6 1.5]    label: [[[0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "dataset = tf.data.Dataset.list_files(['iris_setosa.csv',\n",
    "                                      'iris_versic.csv'],\n",
    "                                      shuffle=False)\n",
    "dataset = dataset.interleave(get_csv_dataset,\n",
    "                             cycle_length=2,\n",
    "                             block_length=1,\n",
    "                             num_parallel_calls=2)\n",
    "dataset = dataset.shard(hvd.size(), hvd.rank())\n",
    "dataset = dataset.map(lambda *row: parse_columns(*row, classes=['setosa', 'virginica', 'versicolor']))\n",
    "\n",
    "for x, y in dataset.take(5):\n",
    "    print(f'features: {x}    label: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-multigpu",
   "language": "python",
   "name": "tf-multigpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
