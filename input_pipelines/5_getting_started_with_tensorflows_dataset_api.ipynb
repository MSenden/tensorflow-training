{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with TensorFlow's `Dataset` API (continuation)\n",
    "\n",
    "In this notebook we continue on how to divide the dataset over the ranks in distributed training. We will combine sharding and interleaving.\n",
    "\n",
    "The following steps were done on one of the previous notebooks. If necessary they can be run again on a new cell.\n",
    "```bash\n",
    "wget https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_setosa.csv\n",
    "grep setosa iris.csv >> iris_setosa.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_versic.csv\n",
    "grep versicolor iris.csv >> iris_versic.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_virgin.csv\n",
    "grep virginica iris.csv >> iris_virgin.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipcmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPCluster is ready! (6 seconds)\n"
     ]
    }
   ],
   "source": [
    "%ipcluster start -n 2 --mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow.keras as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:2]: \u001b[0m'2.3.0'"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2020-09-03T13:21:00.249463",
      "data": {},
      "engine_id": 1,
      "engine_uuid": "f84a3531-f7b781c155b01d8e1b5d341c",
      "error": null,
      "execute_input": "tf.version.VERSION\n",
      "execute_result": {
       "data": {
        "text/plain": "'2.3.0'"
       },
       "execution_count": 2,
       "metadata": {}
      },
      "follow": [],
      "msg_id": "060428bc-6ff1f4ead9a2dc84a96abe27_3",
      "outputs": [],
      "received": "2020-09-03T13:21:00.250685",
      "started": "2020-09-03T13:21:00.243190",
      "status": "ok",
      "stderr": "",
      "stdout": "",
      "submitted": "2020-09-03T13:21:00.241522"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --target 1\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def parse_columns(*row, classes):\n",
    "    \"\"\"Convert the string classes to one-hot encoded:\n",
    "    setosa     -> [1, 0, 0]\n",
    "    virginica  -> [0, 1, 0]\n",
    "    versicolor -> [0, 0, 1]\n",
    "    \n",
    "    and stack all features on a single tensor.\n",
    "    \"\"\"\n",
    "    features = tf.stack(row[:4])\n",
    "    label_int = tf.where(tf.equal(classes, row[4]))\n",
    "    label = tf.one_hot(label_int, 3)\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def get_csv_dataset(filename):\n",
    "    return tf.data.experimental.CsvDataset(filename, header=True,\n",
    "                                           record_defaults=[tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "hvd.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combinig sharding and interleaving it's possible to do things like making every file to be read by only one node, instead of multiple nodes accessing it. We do that by sharding after `list_files`. Here we `interleave` only to pass the file name to the `get_csv_dataset` function. In other setups, interleave can be used to mix datasets within the same worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "    0 features: [5.1 3.5 1.4 0.2]    label: [[[1. 0. 0.]]]\n",
      "    1 features: [4.9 3.  1.4 0.2]    label: [[[1. 0. 0.]]]\n",
      "    2 features: [4.7 3.2 1.3 0.2]    label: [[[1. 0. 0.]]]\n",
      "    3 features: [4.6 3.1 1.5 0.2]    label: [[[1. 0. 0.]]]\n",
      "    4 features: [5.  3.6 1.4 0.2]    label: [[[1. 0. 0.]]]\n",
      "    5 features: [5.4 3.9 1.7 0.4]    label: [[[1. 0. 0.]]]\n",
      "    6 features: [4.6 3.4 1.4 0.3]    label: [[[1. 0. 0.]]]\n",
      "    7 features: [5.  3.4 1.5 0.2]    label: [[[1. 0. 0.]]]\n",
      "    8 features: [4.4 2.9 1.4 0.2]    label: [[[1. 0. 0.]]]\n",
      "    9 features: [4.9 3.1 1.5 0.1]    label: [[[1. 0. 0.]]]\n",
      "   10 features: [5.4 3.7 1.5 0.2]    label: [[[1. 0. 0.]]]\n",
      "   11 features: [4.8 3.4 1.6 0.2]    label: [[[1. 0. 0.]]]\n",
      "   12 features: [4.8 3.  1.4 0.1]    label: [[[1. 0. 0.]]]\n",
      "   13 features: [4.3 3.  1.1 0.1]    label: [[[1. 0. 0.]]]\n",
      "   14 features: [5.8 4.  1.2 0.2]    label: [[[1. 0. 0.]]]\n",
      "   15 features: [5.7 4.4 1.5 0.4]    label: [[[1. 0. 0.]]]\n",
      "   16 features: [5.4 3.9 1.3 0.4]    label: [[[1. 0. 0.]]]\n",
      "   17 features: [5.1 3.5 1.4 0.3]    label: [[[1. 0. 0.]]]\n",
      "   18 features: [5.7 3.8 1.7 0.3]    label: [[[1. 0. 0.]]]\n",
      "   19 features: [5.1 3.8 1.5 0.3]    label: [[[1. 0. 0.]]]\n",
      "   20 features: [5.4 3.4 1.7 0.2]    label: [[[1. 0. 0.]]]\n",
      "   21 features: [5.1 3.7 1.5 0.4]    label: [[[1. 0. 0.]]]\n",
      "   22 features: [4.6 3.6 1.  0.2]    label: [[[1. 0. 0.]]]\n",
      "   23 features: [5.1 3.3 1.7 0.5]    label: [[[1. 0. 0.]]]\n",
      "   24 features: [4.8 3.4 1.9 0.2]    label: [[[1. 0. 0.]]]\n",
      "   25 features: [5.  3.  1.6 0.2]    label: [[[1. 0. 0.]]]\n",
      "   26 features: [5.  3.4 1.6 0.4]    label: [[[1. 0. 0.]]]\n",
      "   27 features: [5.2 3.5 1.5 0.2]    label: [[[1. 0. 0.]]]\n",
      "   28 features: [5.2 3.4 1.4 0.2]    label: [[[1. 0. 0.]]]\n",
      "   29 features: [4.7 3.2 1.6 0.2]    label: [[[1. 0. 0.]]]\n",
      "   30 features: [4.8 3.1 1.6 0.2]    label: [[[1. 0. 0.]]]\n",
      "   31 features: [5.4 3.4 1.5 0.4]    label: [[[1. 0. 0.]]]\n",
      "   32 features: [5.2 4.1 1.5 0.1]    label: [[[1. 0. 0.]]]\n",
      "   33 features: [5.5 4.2 1.4 0.2]    label: [[[1. 0. 0.]]]\n",
      "   34 features: [4.9 3.1 1.5 0.1]    label: [[[1. 0. 0.]]]\n",
      "   35 features: [5.  3.2 1.2 0.2]    label: [[[1. 0. 0.]]]\n",
      "   36 features: [5.5 3.5 1.3 0.2]    label: [[[1. 0. 0.]]]\n",
      "   37 features: [4.9 3.1 1.5 0.1]    label: [[[1. 0. 0.]]]\n",
      "   38 features: [4.4 3.  1.3 0.2]    label: [[[1. 0. 0.]]]\n",
      "   39 features: [5.1 3.4 1.5 0.2]    label: [[[1. 0. 0.]]]\n",
      "   40 features: [5.  3.5 1.3 0.3]    label: [[[1. 0. 0.]]]\n",
      "   41 features: [4.5 2.3 1.3 0.3]    label: [[[1. 0. 0.]]]\n",
      "   42 features: [4.4 3.2 1.3 0.2]    label: [[[1. 0. 0.]]]\n",
      "   43 features: [5.  3.5 1.6 0.6]    label: [[[1. 0. 0.]]]\n",
      "   44 features: [5.1 3.8 1.9 0.4]    label: [[[1. 0. 0.]]]\n",
      "   45 features: [4.8 3.  1.4 0.3]    label: [[[1. 0. 0.]]]\n",
      "   46 features: [5.1 3.8 1.6 0.2]    label: [[[1. 0. 0.]]]\n",
      "   47 features: [4.6 3.2 1.4 0.2]    label: [[[1. 0. 0.]]]\n",
      "   48 features: [5.3 3.7 1.5 0.2]    label: [[[1. 0. 0.]]]\n",
      "   49 features: [5.  3.3 1.4 0.2]    label: [[[1. 0. 0.]]]\n",
      "[stdout:1] \n",
      "    0 features: [7.  3.2 4.7 1.4]    label: [[[0. 0. 1.]]]\n",
      "    1 features: [6.4 3.2 4.5 1.5]    label: [[[0. 0. 1.]]]\n",
      "    2 features: [6.9 3.1 4.9 1.5]    label: [[[0. 0. 1.]]]\n",
      "    3 features: [5.5 2.3 4.  1.3]    label: [[[0. 0. 1.]]]\n",
      "    4 features: [6.5 2.8 4.6 1.5]    label: [[[0. 0. 1.]]]\n",
      "    5 features: [5.7 2.8 4.5 1.3]    label: [[[0. 0. 1.]]]\n",
      "    6 features: [6.3 3.3 4.7 1.6]    label: [[[0. 0. 1.]]]\n",
      "    7 features: [4.9 2.4 3.3 1. ]    label: [[[0. 0. 1.]]]\n",
      "    8 features: [6.6 2.9 4.6 1.3]    label: [[[0. 0. 1.]]]\n",
      "    9 features: [5.2 2.7 3.9 1.4]    label: [[[0. 0. 1.]]]\n",
      "   10 features: [5.  2.  3.5 1. ]    label: [[[0. 0. 1.]]]\n",
      "   11 features: [5.9 3.  4.2 1.5]    label: [[[0. 0. 1.]]]\n",
      "   12 features: [6.  2.2 4.  1. ]    label: [[[0. 0. 1.]]]\n",
      "   13 features: [6.1 2.9 4.7 1.4]    label: [[[0. 0. 1.]]]\n",
      "   14 features: [5.6 2.9 3.6 1.3]    label: [[[0. 0. 1.]]]\n",
      "   15 features: [6.7 3.1 4.4 1.4]    label: [[[0. 0. 1.]]]\n",
      "   16 features: [5.6 3.  4.5 1.5]    label: [[[0. 0. 1.]]]\n",
      "   17 features: [5.8 2.7 4.1 1. ]    label: [[[0. 0. 1.]]]\n",
      "   18 features: [6.2 2.2 4.5 1.5]    label: [[[0. 0. 1.]]]\n",
      "   19 features: [5.6 2.5 3.9 1.1]    label: [[[0. 0. 1.]]]\n",
      "   20 features: [5.9 3.2 4.8 1.8]    label: [[[0. 0. 1.]]]\n",
      "   21 features: [6.1 2.8 4.  1.3]    label: [[[0. 0. 1.]]]\n",
      "   22 features: [6.3 2.5 4.9 1.5]    label: [[[0. 0. 1.]]]\n",
      "   23 features: [6.1 2.8 4.7 1.2]    label: [[[0. 0. 1.]]]\n",
      "   24 features: [6.4 2.9 4.3 1.3]    label: [[[0. 0. 1.]]]\n",
      "   25 features: [6.6 3.  4.4 1.4]    label: [[[0. 0. 1.]]]\n",
      "   26 features: [6.8 2.8 4.8 1.4]    label: [[[0. 0. 1.]]]\n",
      "   27 features: [6.7 3.  5.  1.7]    label: [[[0. 0. 1.]]]\n",
      "   28 features: [6.  2.9 4.5 1.5]    label: [[[0. 0. 1.]]]\n",
      "   29 features: [5.7 2.6 3.5 1. ]    label: [[[0. 0. 1.]]]\n",
      "   30 features: [5.5 2.4 3.8 1.1]    label: [[[0. 0. 1.]]]\n",
      "   31 features: [5.5 2.4 3.7 1. ]    label: [[[0. 0. 1.]]]\n",
      "   32 features: [5.8 2.7 3.9 1.2]    label: [[[0. 0. 1.]]]\n",
      "   33 features: [6.  2.7 5.1 1.6]    label: [[[0. 0. 1.]]]\n",
      "   34 features: [5.4 3.  4.5 1.5]    label: [[[0. 0. 1.]]]\n",
      "   35 features: [6.  3.4 4.5 1.6]    label: [[[0. 0. 1.]]]\n",
      "   36 features: [6.7 3.1 4.7 1.5]    label: [[[0. 0. 1.]]]\n",
      "   37 features: [6.3 2.3 4.4 1.3]    label: [[[0. 0. 1.]]]\n",
      "   38 features: [5.6 3.  4.1 1.3]    label: [[[0. 0. 1.]]]\n",
      "   39 features: [5.5 2.5 4.  1.3]    label: [[[0. 0. 1.]]]\n",
      "   40 features: [5.5 2.6 4.4 1.2]    label: [[[0. 0. 1.]]]\n",
      "   41 features: [6.1 3.  4.6 1.4]    label: [[[0. 0. 1.]]]\n",
      "   42 features: [5.8 2.6 4.  1.2]    label: [[[0. 0. 1.]]]\n",
      "   43 features: [5.  2.3 3.3 1. ]    label: [[[0. 0. 1.]]]\n",
      "   44 features: [5.6 2.7 4.2 1.3]    label: [[[0. 0. 1.]]]\n",
      "   45 features: [5.7 3.  4.2 1.2]    label: [[[0. 0. 1.]]]\n",
      "   46 features: [5.7 2.9 4.2 1.3]    label: [[[0. 0. 1.]]]\n",
      "   47 features: [6.2 2.9 4.3 1.3]    label: [[[0. 0. 1.]]]\n",
      "   48 features: [5.1 2.5 3.  1.1]    label: [[[0. 0. 1.]]]\n",
      "   49 features: [5.7 2.8 4.1 1.3]    label: [[[0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "dataset = tf.data.Dataset.list_files(['iris_setosa.csv',\n",
    "                                      'iris_versic.csv'],\n",
    "                                      shuffle=False)\n",
    "dataset = dataset.shard(hvd.size(), hvd.rank())\n",
    "dataset = dataset.interleave(get_csv_dataset,\n",
    "                             cycle_length=1,\n",
    "                             block_length=1,\n",
    "                             num_parallel_calls=1)\n",
    "# dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda *row: parse_columns(*row, classes=['setosa', 'virginica', 'versicolor']))\n",
    "\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    print(f'{i:5} features: {x}    label: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the amount of data is distributed evenly over all workers. Unbalanced workers can hurt performance, convergence or make the program not function correctly.\n",
    "\n",
    "<mark>Exercise</mark>: Add the third csv file `iris_virgin.csv` and see what happens on the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-multigpu",
   "language": "python",
   "name": "tf-multigpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
