{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding input pipeline optimizations\n",
    "\n",
    "This notebooks is based on the tutorial [Better performance with the tf.data API](https://www.tensorflow.org/guide/data_performance) from TensorFlow's documentation.\n",
    "\n",
    "![alt text](imgs/timeline.png \"Title\")\n",
    "\n",
    "Here we are going to use a generator to simulate opening and reading a file to create a schematic timeline of the input pipeline. The time spent in opening the data, reading it, mapping and training will be simulated with `time.sleep()`. We are going to generate a trace event json file and visualize it with Chrome's tracing utility (chrome://tracing/).\n",
    "\n",
    "The idea now is to loop over the dataset simulating the training and apply `map` with and wihout `num_parallel_calls`, add `cache` and `prefetch` transformations and see what happens. The 'training' produces the file `tfdata.json` that we need to copy to the computer where we have Chrome.\n",
    "\n",
    "More info:\n",
    " * [Trace Event Format](https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to set the reference of the timer to a small value\n",
    "# as sometimes the `time.perf_counter()` is to large for `tf.float32`\n",
    "GLOBAL_TIME_BASELINE = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(num_samples):\n",
    "    # Opening the file\n",
    "    open_enter = time.perf_counter() - GLOBAL_TIME_BASELINE\n",
    "    time.sleep(0.03)\n",
    "    open_elapsed = time.perf_counter() - open_enter - GLOBAL_TIME_BASELINE\n",
    "\n",
    "    for sample_idx in range(num_samples):\n",
    "        # Reading data (line, record) from the file\n",
    "        read_enter = time.perf_counter() - GLOBAL_TIME_BASELINE\n",
    "        time.sleep(0.02)\n",
    "        read_elapsed = time.perf_counter() - read_enter - GLOBAL_TIME_BASELINE\n",
    "\n",
    "        yield (\n",
    "            [(\"Open\",), (\"Read\",)],\n",
    "            [(open_enter, open_elapsed), (read_enter, read_elapsed)],\n",
    "        )\n",
    "        # Negative values will be filtered out\n",
    "        open_enter, open_elapsed = -1., -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The map function needs to be wrapped using `tf.py_function`\n",
    "# to prevent auto-graph from compiling it\n",
    "# If the function is compiled, then the starting time of the\n",
    "# map operation will be allways the same\n",
    "def map_decorator(func):\n",
    "    def wrapper(steps, times):\n",
    "        return tf.py_function(\n",
    "            func,\n",
    "            inp=(steps, times),\n",
    "            Tout=(steps.dtype, times.dtype)\n",
    "        )\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@map_decorator\n",
    "def mapping(*sample):\n",
    "    map_start = time.perf_counter() - GLOBAL_TIME_BASELINE\n",
    "    time.sleep(0.03)\n",
    "    map_elapsed = time.perf_counter() - map_start - GLOBAL_TIME_BASELINE\n",
    "    return (\n",
    "        [sample[0][0], sample[0][1], ('Map',)],\n",
    "        [(sample[1][0][0], sample[1][0][1]),\n",
    "         (sample[1][1][0], sample[1][1][1]),\n",
    "         (map_start, map_elapsed)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "dataset = tf.data.Dataset.from_generator(generator,\n",
    "                                         output_types=(tf.dtypes.string, tf.dtypes.float32),\n",
    "                                         output_shapes=((2, 1), (2, 2)),\n",
    "                                         args=(num_samples,)\n",
    "                                        )\n",
    "# dataset = dataset.shuffle(4)\n",
    "# dataset = dataset.map(mapping, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# dataset = dataset.cache()\n",
    "# dataset = dataset.prefetch(12)  #, tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = 1e6\n",
    "trace_events = {'traceEvents': []}\n",
    "for ops, times in dataset:\n",
    "    for op, t in zip(ops, times):\n",
    "        if t.numpy()[0] <= 0:\n",
    "            continue\n",
    "\n",
    "        name = f'{op[0]}'[2:-1]  # from tf it comes as b'open'\n",
    "        ts = t.numpy()[0]\n",
    "        dur = t.numpy()[1]\n",
    "        # includes open, read and map:\n",
    "        trace_events['traceEvents'].append({\"pid\": 1,\n",
    "                                            \"tid\": name,\n",
    "                                            \"ts\": int(ts * us),\n",
    "                                            \"dur\": int(dur * us),\n",
    "                                            \"ph\":\"X\",\n",
    "                                            \"name\": name})\n",
    "    train_start = time.perf_counter() - GLOBAL_TIME_BASELINE\n",
    "    train_dur = 0.01\n",
    "    time.sleep(train_dur)\n",
    "    trace_events['traceEvents'].append({\"pid\": 1,\n",
    "                                        \"tid\": 'Train',\n",
    "                                        \"ts\": int(train_start * us),\n",
    "                                        \"dur\": int(train_dur * us),\n",
    "                                        \"ph\":\"X\",\n",
    "                                        \"name\": 'Train'})\n",
    "\n",
    "with open('tfdata.json', 'w') as f:\n",
    "    json.dump(trace_events, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-multigpu",
   "language": "python",
   "name": "tf-multigpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
